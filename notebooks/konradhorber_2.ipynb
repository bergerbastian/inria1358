{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/bastianberger/inria1358')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.open('/home/jupyter/bastianberger/inria1358/raw_data/patches512/train/images/austin28.tif__4__1.png')\n",
    "np.array(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Checking local data...\u001b[0m\n",
      "0 Patches found for X_train\n",
      "0 Patches found for y_train\n",
      "0 Patches found for X_test\n",
      "❗ Train folders contain 0 images\n",
      "❗ Test folders contain 0 images\n",
      "\u001b[34m\n",
      "Iterating through folder: train/images\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [2:36:43<00:00, 52.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Iterating through folder: train/gt\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [06:40<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Iterating through folder: test/images\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 140/180 [2:00:20<34:22, 51.57s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:515\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    516\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmake_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jupyter/bastianberger/inria1358/raw_data/AerialImageDataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jupyter/bastianberger/inria1358/raw_data/patches500\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bastianberger/inria1358/ML/data.py:94\u001b[0m, in \u001b[0;36mmake_patches\u001b[0;34m(source_path, save_path, image_size, image_type, max_files)\u001b[0m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m             dimensions = image_size if subfolder == \"gt\" else (image_size[0],image_size[1],3)\n\u001b[0;32m---> 94\u001b[0m             file_count = 0\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m             files = os.listdir(load_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2413\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2413\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1398\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:519\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    517\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 519\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    521\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:538\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    539\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_patches(source_path='/home/jupyter/bastianberger/inria1358/raw_data/AerialImageDataset',\n",
    "             save_path='/home/jupyter/bastianberger/inria1358/raw_data/patches500',\n",
    "             image_size=(500,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_512(source_path: str, save_path: str, image_size=(200,200), image_type=\"png\", max_files = None):\n",
    "    \"\"\"Function to create patches from the original images, in order to be able to feed them to the model\n",
    "        Args:\n",
    "            source_path: path to the original dataset\n",
    "            save_path: path where the patches should be saved\n",
    "            image_size: (width, height) of patches\n",
    "            image_type: file type that the patches should be outputted in\n",
    "            max_files: maximum number of images to process per subfolder (selected randomly), None = no limit\n",
    "    \"\"\"\n",
    "\n",
    "    # Checking if directories exist, if not, create them\n",
    "    for folder in ['train','test']:\n",
    "        subfolders = ['gt', 'images'] if folder == \"train\" else ['images']\n",
    "        for subfolder in subfolders:\n",
    "            load_path = f'{source_path}/{folder}/{subfolder}'\n",
    "            save_path_n = f'{save_path}/{folder}/{subfolder}'\n",
    "            # Check whether the specified path exists or not\n",
    "            isExist = os.path.exists(save_path_n)\n",
    "            if not isExist:\n",
    "                os.makedirs(save_path_n)\n",
    "\n",
    "    # Checking the local data\n",
    "    sets = []\n",
    "    print(Fore.BLUE + \"\\nChecking local data...\" + Style.RESET_ALL)\n",
    "\n",
    "    # Get # of images from save path\n",
    "    train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "    train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "    test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "    print(f\"{train_images} Patches found for X_train\")\n",
    "    print(f\"{train_gt} Patches found for y_train\")\n",
    "    print(f\"{test_images} Patches found for X_test\")\n",
    "\n",
    "    # Check if >0 images are there for each set (meaning we have data) otherwise add it to the sets that we want to patch\n",
    "    if train_images == 0 or train_gt == 0:\n",
    "        print(\"❗ Train folders contain 0 images\")\n",
    "        sets.append(\"train\")\n",
    "    else:\n",
    "        print(\"ℹ️ Train patches already exist\")\n",
    "\n",
    "    if test_images == 0:\n",
    "        print(\"❗ Test folders contain 0 images\")\n",
    "        sets.append(\"test\")\n",
    "    else:\n",
    "        print(\"ℹ️ Test patches already exist\")\n",
    "\n",
    "\n",
    "    # Iterate through the sets\n",
    "    for set in sets:\n",
    "\n",
    "        # Subfolders to iterate through, if the set is train, iterate also iterate through GT folder\n",
    "        subfolders = ['images']\n",
    "        if set == 'train':\n",
    "            subfolders.append('gt')\n",
    "\n",
    "        # Iterate through each subfolder\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            load_path = f'{source_path}/{set}/{subfolder}'\n",
    "            save_path_n = f'{save_path}/{set}/{subfolder}'\n",
    "\n",
    "            print(Fore.BLUE + f\"\\nIterating through folder: {set}/{subfolder}\" + Style.RESET_ALL)\n",
    "\n",
    "            dimensions = image_size if subfolder == \"gt\" else (image_size[0],image_size[1],3)\n",
    "            file_count = 0\n",
    "\n",
    "            files = os.listdir(load_path)\n",
    "\n",
    "            if max_files is not None:\n",
    "                files = random.sample(files, max_files)\n",
    "\n",
    "            for filename in tqdm(files):\n",
    "                im = Image.open(f'{load_path}/{filename}')\n",
    "                im = im.resize((512,512))\n",
    "                im.save(f'{save_path_n}/{filename}.{image_type}'\n",
    "\n",
    "    # Final Output\n",
    "    # Get # of images from save path\n",
    "    train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "    train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "    test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "    if len(sets) > 0:\n",
    "        print(Fore.BLUE + f\"\\nCounting local images...\" + Style.RESET_ALL)\n",
    "        print(f\"{train_images} Patches found for X_train\")\n",
    "        print(f\"{train_gt} Patches found for y_train\")\n",
    "        print(f\"{test_images} Patches found for X_test\")\n",
    "    print(\"✅ Patches loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_512(source_path='/home/jupyter/bastianberger/inria1358/raw_data/patches500',\n",
    "         save_path='/home/jupyter/bastianberger/inria1358/raw_data/patches512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 09:09:11.196797: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 09:09:16.030216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-25 09:09:16.031237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-25 09:09:16.031249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 Patches found for X_train\n",
      "18000 Patches found for y_train\n",
      "18000 Patches found for X_test\n",
      "ℹ️ Train patches already exist\n",
      "ℹ️ Test patches already exist\n",
      "✅ Patches loaded\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from patchify import patchify\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from colorama import Fore, Style\n",
    "from multiprocessing import Pool\n",
    "source_path='/home/jupyter/bastianberger/inria1358/raw_data/AerialImageDataset'\n",
    "save_path='/home/jupyter/bastianberger/inria1358/raw_data/test'\n",
    "image_size=(500,500)\n",
    "image_type='png'\n",
    "max_files=None\n",
    "\n",
    "# Checking if directories exist, if not, create them\n",
    "for folder in ['train','test']:\n",
    "    subfolders = ['gt', 'images'] if folder == \"train\" else ['images']\n",
    "    for subfolder in subfolders:\n",
    "        load_path = f'{source_path}/{folder}/{subfolder}'\n",
    "        save_path_n = f'{save_path}/{folder}/{subfolder}'\n",
    "        # Check whether the specified path exists or not\n",
    "        isExist = os.path.exists(save_path_n)\n",
    "        if not isExist:\n",
    "            os.makedirs(save_path_n)\n",
    "\n",
    "# Checking the local data\n",
    "sets = []\n",
    "\n",
    "# Get # of images from save path\n",
    "train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "print(f\"{train_images} Patches found for X_train\")\n",
    "print(f\"{train_gt} Patches found for y_train\")\n",
    "print(f\"{test_images} Patches found for X_test\")\n",
    "\n",
    "# Check if >0 images are there for each set (meaning we have data) otherwise add it to the sets that we want to patch\n",
    "if train_images == 0 or train_gt == 0:\n",
    "    print(\"❗ Train folders contain 0 images\")\n",
    "    sets.append(\"train\")\n",
    "else:\n",
    "    print(\"ℹ️ Train patches already exist\")\n",
    "\n",
    "if test_images == 0:\n",
    "    print(\"❗ Test folders contain 0 images\")\n",
    "    sets.append(\"test\")\n",
    "else:\n",
    "    print(\"ℹ️ Test patches already exist\")\n",
    "\n",
    "\n",
    "# Iterate through the sets\n",
    "for set in sets:\n",
    "\n",
    "    # Subfolders to iterate through, if the set is train, iterate also iterate through GT folder\n",
    "    subfolders = ['images']\n",
    "    if set == 'train':\n",
    "        subfolders.append('gt')\n",
    "\n",
    "    # Iterate through each subfolder\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        load_path = f'{source_path}/{set}/{subfolder}'\n",
    "        save_path_n = f'{save_path}/{set}/{subfolder}'\n",
    "\n",
    "        dimensions = image_size if subfolder == \"gt\" else (image_size[0],image_size[1],3)\n",
    "        file_count = 0\n",
    "\n",
    "        files = os.listdir(load_path)\n",
    "\n",
    "        if max_files is not None:\n",
    "            files = random.sample(files, max_files)\n",
    "\n",
    "        def process_patchify(filename, load_path, save_path_n, subfolder, image_type, dimensions):\n",
    "            im = Image.open(f'{load_path}/{filename}')\n",
    "            imarray = np.array(im)\n",
    "\n",
    "            # Assuming patchify function is imported\n",
    "            patches = patchify(imarray, dimensions, step=500)\n",
    "\n",
    "            for i, row in enumerate(patches):\n",
    "                for j, col in enumerate(row):\n",
    "                    if subfolder == \"gt\":\n",
    "                        im = Image.fromarray(col)\n",
    "                    else:\n",
    "                        im = Image.fromarray(col[0])\n",
    "\n",
    "                    im.save(f'{save_path_n}/{filename}__{i}__{j}.{image_type}')\n",
    "            \n",
    "        def wrapper_function(filename):\n",
    "            process_patchify(filename, load_path, save_path_n, subfolder, image_type, dimensions)\n",
    "\n",
    "        # ... (Your code before this)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = Pool()\n",
    "            for _ in tqdm(pool.imap_unordered(wrapper_function, files), total=len(files)):\n",
    "                pass\n",
    "\n",
    "# Final Output\n",
    "# Get # of images from save path\n",
    "train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "if len(sets) > 0:\n",
    "    print(f\"{train_images} Patches found for X_train\")\n",
    "    print(f\"{train_gt} Patches found for y_train\")\n",
    "    print(f\"{test_images} Patches found for X_test\")\n",
    "print(\"✅ Patches loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Checking local data...\u001b[0m\n",
      "18000 Patches found for X_train\n",
      "18000 Patches found for y_train\n",
      "18000 Patches found for X_test\n",
      "ℹ️ Train patches already exist\n",
      "ℹ️ Test patches already exist\n",
      "✅ Patches loaded\n"
     ]
    }
   ],
   "source": [
    "source_path='/home/jupyter/bastianberger/inria1358/raw_data/test'\n",
    "save_path='/home/jupyter/bastianberger/inria1358/raw_data/patches512'\n",
    "image_size=(500,500)\n",
    "image_type='png'\n",
    "max_files=None\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from patchify import patchify\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from colorama import Fore, Style\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Checking if directories exist, if not, create them\n",
    "for folder in ['train','test']:\n",
    "    subfolders = ['gt', 'images'] if folder == \"train\" else ['images']\n",
    "    for subfolder in subfolders:\n",
    "        load_path = f'{source_path}/{folder}/{subfolder}'\n",
    "        save_path_n = f'{save_path}/{folder}/{subfolder}'\n",
    "        # Check whether the specified path exists or not\n",
    "        isExist = os.path.exists(save_path_n)\n",
    "        if not isExist:\n",
    "            os.makedirs(save_path_n)\n",
    "\n",
    "# Checking the local data\n",
    "sets = []\n",
    "print(Fore.BLUE + \"\\nChecking local data...\" + Style.RESET_ALL)\n",
    "\n",
    "# Get # of images from save path\n",
    "train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "print(f\"{train_images} Patches found for X_train\")\n",
    "print(f\"{train_gt} Patches found for y_train\")\n",
    "print(f\"{test_images} Patches found for X_test\")\n",
    "\n",
    "# Check if >0 images are there for each set (meaning we have data) otherwise add it to the sets that we want to patch\n",
    "if train_images == 0 or train_gt == 0:\n",
    "    print(\"❗ Train folders contain 0 images\")\n",
    "    sets.append(\"train\")\n",
    "else:\n",
    "    print(\"ℹ️ Train patches already exist\")\n",
    "\n",
    "if test_images == 0:\n",
    "    print(\"❗ Test folders contain 0 images\")\n",
    "    sets.append(\"test\")\n",
    "else:\n",
    "    print(\"ℹ️ Test patches already exist\")\n",
    "\n",
    "\n",
    "# Iterate through the sets\n",
    "for set in sets:\n",
    "\n",
    "    # Subfolders to iterate through, if the set is train, iterate also iterate through GT folder\n",
    "    subfolders = ['images']\n",
    "    if set == 'train':\n",
    "        subfolders.append('gt')\n",
    "\n",
    "    # Iterate through each subfolder\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        load_path = f'{source_path}/{set}/{subfolder}'\n",
    "        save_path_n = f'{save_path}/{set}/{subfolder}'\n",
    "\n",
    "        print(Fore.BLUE + f\"\\nIterating through folder: {set}/{subfolder}\" + Style.RESET_ALL)\n",
    "\n",
    "        dimensions = image_size if subfolder == \"gt\" else (image_size[0],image_size[1],3)\n",
    "        file_count = 0\n",
    "\n",
    "        files = os.listdir(load_path)\n",
    "\n",
    "        if max_files is not None:\n",
    "            files = random.sample(files, max_files)\n",
    "\n",
    "        def wrapper_function2(filename):\n",
    "            im = Image.open(f'{load_path}/{filename}')\n",
    "            im = im.resize((512,512))\n",
    "            im.save(f'{save_path_n}/{filename}')\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = Pool()\n",
    "            pool.map(wrapper_function2, files)\n",
    "\n",
    "# Final Output\n",
    "# Get # of images from save path\n",
    "train_images = len(os.listdir(f\"{save_path}/train/images\"))\n",
    "train_gt = len(os.listdir(f\"{save_path}/train/gt\"))\n",
    "test_images = len(os.listdir(f\"{save_path}/test/images\"))\n",
    "\n",
    "if len(sets) > 0:\n",
    "    print(Fore.BLUE + f\"\\nCounting local images...\" + Style.RESET_ALL)\n",
    "    print(f\"{train_images} Patches found for X_train\")\n",
    "    print(f\"{train_gt} Patches found for y_train\")\n",
    "    print(f\"{test_images} Patches found for X_test\")\n",
    "print(\"✅ Patches loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

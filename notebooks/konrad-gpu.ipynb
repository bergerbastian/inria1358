{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6be336a-225c-4e14-a35f-ad8b39b892e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35547f5c-65d1-41e9-87e1-d4f5fbb4dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "sys.path.append('/home/jupyter/bastianberger/inria1358')\n",
    "from ML.preprocessing import create_datasets\n",
    "from ML.model import make_model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837b314a-503f-4a41-b2ef-475f895de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_path = '/home/jupyter/bastianberger/inria1358/raw_data/patches2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6701c1-d9fd-4d57-b8f8-8b8c6633c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:37:45.030782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:45.251469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:45.254651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:45.263401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 16:37:45.267945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:45.270706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:45.273316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:47.167594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:47.181204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:47.182900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-22 16:37:47.188574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test_batches with 11250 images (176 batches) created\n",
      "✅ Train_batches with 7087 images (111 batches) created\n",
      "✅ Val_batches with 3038 images (48 batches) created\n"
     ]
    }
   ],
   "source": [
    "test_batches, train_batches, val_batches = create_datasets(patches_path, data_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a7332-d5ae-44d8-989d-c5349100b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UNET Model initialized\n",
      "✅ Model compiled\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:39:49.922923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-09-22 16:40:08.205280: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:08.205341: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:08.569144: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:08.569210: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:09.628032: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:09.628094: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:13.054634: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:40:13.054704: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94/111 [========================>.....] - ETA: 19s - loss: 0.4066 - accuracy: 0.8465 - mean_io_u_1: 0.4216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:42:07.242415: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/111 [============================>.] - ETA: 1s - loss: 0.3940 - accuracy: 0.8490 - mean_io_u_1: 0.4208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:42:34.508896: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-09-22 16:42:34.508979: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 195s 1s/step - loss: 0.3934 - accuracy: 0.8491 - mean_io_u_1: 0.4208 - val_loss: 0.3059 - val_accuracy: 0.8604 - val_mean_io_u_1: 0.4222\n",
      "Epoch 2/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2989 - accuracy: 0.8724 - mean_io_u_1: 0.4208 - val_loss: 0.2939 - val_accuracy: 0.8748 - val_mean_io_u_1: 0.4222\n",
      "Epoch 3/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2740 - accuracy: 0.8862 - mean_io_u_1: 0.4208 - val_loss: 0.2540 - val_accuracy: 0.8968 - val_mean_io_u_1: 0.4233\n",
      "Epoch 4/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2598 - accuracy: 0.8937 - mean_io_u_1: 0.4208 - val_loss: 0.2784 - val_accuracy: 0.8836 - val_mean_io_u_1: 0.4222\n",
      "Epoch 5/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2511 - accuracy: 0.8982 - mean_io_u_1: 0.4208 - val_loss: 0.2503 - val_accuracy: 0.8951 - val_mean_io_u_1: 0.4224\n",
      "Epoch 6/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2408 - accuracy: 0.9031 - mean_io_u_1: 0.4208 - val_loss: 0.2353 - val_accuracy: 0.9052 - val_mean_io_u_1: 0.4219\n",
      "Epoch 7/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2331 - accuracy: 0.9062 - mean_io_u_1: 0.4208 - val_loss: 0.2311 - val_accuracy: 0.9086 - val_mean_io_u_1: 0.4214\n",
      "Epoch 8/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.2180 - accuracy: 0.9129 - mean_io_u_1: 0.4208 - val_loss: 0.2113 - val_accuracy: 0.9157 - val_mean_io_u_1: 0.4230\n",
      "Epoch 9/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2132 - accuracy: 0.9153 - mean_io_u_1: 0.4208 - val_loss: 0.2084 - val_accuracy: 0.9162 - val_mean_io_u_1: 0.4212\n",
      "Epoch 10/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2091 - accuracy: 0.9166 - mean_io_u_1: 0.4208 - val_loss: 0.2158 - val_accuracy: 0.9152 - val_mean_io_u_1: 0.4223\n",
      "Epoch 11/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.2036 - accuracy: 0.9187 - mean_io_u_1: 0.4208 - val_loss: 0.1944 - val_accuracy: 0.9223 - val_mean_io_u_1: 0.4218\n",
      "Epoch 12/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.1991 - accuracy: 0.9213 - mean_io_u_1: 0.4208 - val_loss: 0.1890 - val_accuracy: 0.9263 - val_mean_io_u_1: 0.4216\n",
      "Epoch 13/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1901 - accuracy: 0.9245 - mean_io_u_1: 0.4208 - val_loss: 0.1852 - val_accuracy: 0.9266 - val_mean_io_u_1: 0.4229\n",
      "Epoch 14/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.1851 - accuracy: 0.9266 - mean_io_u_1: 0.4208 - val_loss: 0.1966 - val_accuracy: 0.9218 - val_mean_io_u_1: 0.4233\n",
      "Epoch 15/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1871 - accuracy: 0.9259 - mean_io_u_1: 0.4208 - val_loss: 0.1768 - val_accuracy: 0.9309 - val_mean_io_u_1: 0.4222\n",
      "Epoch 16/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1776 - accuracy: 0.9301 - mean_io_u_1: 0.4208 - val_loss: 0.1827 - val_accuracy: 0.9281 - val_mean_io_u_1: 0.4223\n",
      "Epoch 17/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1738 - accuracy: 0.9315 - mean_io_u_1: 0.4208 - val_loss: 0.1724 - val_accuracy: 0.9328 - val_mean_io_u_1: 0.4207\n",
      "Epoch 18/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1713 - accuracy: 0.9324 - mean_io_u_1: 0.4208 - val_loss: 0.1707 - val_accuracy: 0.9331 - val_mean_io_u_1: 0.4226\n",
      "Epoch 19/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1709 - accuracy: 0.9326 - mean_io_u_1: 0.4208 - val_loss: 0.1731 - val_accuracy: 0.9315 - val_mean_io_u_1: 0.4220\n",
      "Epoch 20/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1653 - accuracy: 0.9351 - mean_io_u_1: 0.4208 - val_loss: 0.1554 - val_accuracy: 0.9388 - val_mean_io_u_1: 0.4219\n",
      "Epoch 21/100\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.1589 - accuracy: 0.9374 - mean_io_u_1: 0.4208 - val_loss: 0.1550 - val_accuracy: 0.9394 - val_mean_io_u_1: 0.4224\n",
      "Epoch 22/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1540 - accuracy: 0.9396 - mean_io_u_1: 0.4208 - val_loss: 0.1539 - val_accuracy: 0.9399 - val_mean_io_u_1: 0.4221\n",
      "Epoch 23/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1497 - accuracy: 0.9413 - mean_io_u_1: 0.4208 - val_loss: 0.1589 - val_accuracy: 0.9383 - val_mean_io_u_1: 0.4215\n",
      "Epoch 24/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1465 - accuracy: 0.9429 - mean_io_u_1: 0.4208 - val_loss: 0.1381 - val_accuracy: 0.9462 - val_mean_io_u_1: 0.4222\n",
      "Epoch 25/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1446 - accuracy: 0.9434 - mean_io_u_1: 0.4208 - val_loss: 0.1375 - val_accuracy: 0.9464 - val_mean_io_u_1: 0.4224\n",
      "Epoch 26/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1412 - accuracy: 0.9449 - mean_io_u_1: 0.4208 - val_loss: 0.1346 - val_accuracy: 0.9476 - val_mean_io_u_1: 0.4220\n",
      "Epoch 27/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1321 - accuracy: 0.9487 - mean_io_u_1: 0.4208 - val_loss: 0.1285 - val_accuracy: 0.9507 - val_mean_io_u_1: 0.4222\n",
      "Epoch 28/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1315 - accuracy: 0.9488 - mean_io_u_1: 0.4208 - val_loss: 0.1254 - val_accuracy: 0.9512 - val_mean_io_u_1: 0.4212\n",
      "Epoch 29/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1284 - accuracy: 0.9501 - mean_io_u_1: 0.4208 - val_loss: 0.1353 - val_accuracy: 0.9473 - val_mean_io_u_1: 0.4228\n",
      "Epoch 30/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1211 - accuracy: 0.9533 - mean_io_u_1: 0.4209 - val_loss: 0.1141 - val_accuracy: 0.9559 - val_mean_io_u_1: 0.4226\n",
      "Epoch 31/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1158 - accuracy: 0.9551 - mean_io_u_1: 0.4209 - val_loss: 0.1125 - val_accuracy: 0.9567 - val_mean_io_u_1: 0.4225\n",
      "Epoch 32/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1117 - accuracy: 0.9568 - mean_io_u_1: 0.4209 - val_loss: 0.1084 - val_accuracy: 0.9580 - val_mean_io_u_1: 0.4225\n",
      "Epoch 33/100\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.1116 - accuracy: 0.9568 - mean_io_u_1: 0.4210 - val_loss: 0.1348 - val_accuracy: 0.9485 - val_mean_io_u_1: 0.4214\n",
      "Epoch 34/100\n",
      " 65/111 [================>.............] - ETA: 55s - loss: 0.1062 - accuracy: 0.9589 - mean_io_u_1: 0.4224"
     ]
    }
   ],
   "source": [
    "model_unet_point1, history_unet_point1 = make_model(train_batches, val_batches, input_shape=(200,200,3), model_name='unet', epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9877b08-0bbe-4023-869b-559b75082f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history_unet_point1.history)\n",
    "hist_df.to_csv('./logs/histories/unet_point1.csv')\n",
    "model_unet_point1.save('.logs/models/unet_point1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1c5a61-6a78-4b42-ae72-48ed049cf644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>mean_io_u_1</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_mean_io_u_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.393351</td>\n",
       "      <td>0.849080</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.305916</td>\n",
       "      <td>0.860398</td>\n",
       "      <td>0.422161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.298917</td>\n",
       "      <td>0.872365</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.293903</td>\n",
       "      <td>0.874807</td>\n",
       "      <td>0.422168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.274030</td>\n",
       "      <td>0.886187</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.254023</td>\n",
       "      <td>0.896829</td>\n",
       "      <td>0.423341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.259818</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.278415</td>\n",
       "      <td>0.883573</td>\n",
       "      <td>0.422176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.251109</td>\n",
       "      <td>0.898171</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.250251</td>\n",
       "      <td>0.895070</td>\n",
       "      <td>0.422361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.240798</td>\n",
       "      <td>0.903103</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.235326</td>\n",
       "      <td>0.905231</td>\n",
       "      <td>0.421864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.233087</td>\n",
       "      <td>0.906246</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.231087</td>\n",
       "      <td>0.908614</td>\n",
       "      <td>0.421371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.217962</td>\n",
       "      <td>0.912880</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.915719</td>\n",
       "      <td>0.422976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.213215</td>\n",
       "      <td>0.915348</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.208404</td>\n",
       "      <td>0.916180</td>\n",
       "      <td>0.421225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.209126</td>\n",
       "      <td>0.916607</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.215834</td>\n",
       "      <td>0.915227</td>\n",
       "      <td>0.422285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.203553</td>\n",
       "      <td>0.918740</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.194443</td>\n",
       "      <td>0.922298</td>\n",
       "      <td>0.421762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.199113</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.188993</td>\n",
       "      <td>0.926336</td>\n",
       "      <td>0.421555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.190084</td>\n",
       "      <td>0.924501</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.185193</td>\n",
       "      <td>0.926637</td>\n",
       "      <td>0.422870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.185114</td>\n",
       "      <td>0.926576</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.196635</td>\n",
       "      <td>0.921784</td>\n",
       "      <td>0.423311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.187143</td>\n",
       "      <td>0.925883</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.176811</td>\n",
       "      <td>0.930917</td>\n",
       "      <td>0.422199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.177641</td>\n",
       "      <td>0.930088</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.182740</td>\n",
       "      <td>0.928102</td>\n",
       "      <td>0.422257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.173813</td>\n",
       "      <td>0.931489</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.172365</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.420742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.171297</td>\n",
       "      <td>0.932424</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.170731</td>\n",
       "      <td>0.933085</td>\n",
       "      <td>0.422574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.170946</td>\n",
       "      <td>0.932588</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.931515</td>\n",
       "      <td>0.421975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.165310</td>\n",
       "      <td>0.935062</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.155370</td>\n",
       "      <td>0.938788</td>\n",
       "      <td>0.421878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.158853</td>\n",
       "      <td>0.937438</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.155012</td>\n",
       "      <td>0.939444</td>\n",
       "      <td>0.422438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.153971</td>\n",
       "      <td>0.939650</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.153901</td>\n",
       "      <td>0.939911</td>\n",
       "      <td>0.422108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.941329</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.158899</td>\n",
       "      <td>0.938331</td>\n",
       "      <td>0.421524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.146492</td>\n",
       "      <td>0.942929</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.138055</td>\n",
       "      <td>0.946200</td>\n",
       "      <td>0.422158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.144577</td>\n",
       "      <td>0.943357</td>\n",
       "      <td>0.420776</td>\n",
       "      <td>0.137464</td>\n",
       "      <td>0.946435</td>\n",
       "      <td>0.422440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.141154</td>\n",
       "      <td>0.944884</td>\n",
       "      <td>0.420793</td>\n",
       "      <td>0.134644</td>\n",
       "      <td>0.947564</td>\n",
       "      <td>0.421979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.132149</td>\n",
       "      <td>0.948744</td>\n",
       "      <td>0.420782</td>\n",
       "      <td>0.128493</td>\n",
       "      <td>0.950665</td>\n",
       "      <td>0.422195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.948760</td>\n",
       "      <td>0.420822</td>\n",
       "      <td>0.125392</td>\n",
       "      <td>0.951217</td>\n",
       "      <td>0.421244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.128393</td>\n",
       "      <td>0.950083</td>\n",
       "      <td>0.420850</td>\n",
       "      <td>0.135331</td>\n",
       "      <td>0.947284</td>\n",
       "      <td>0.422809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.121053</td>\n",
       "      <td>0.953270</td>\n",
       "      <td>0.420900</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.955946</td>\n",
       "      <td>0.422581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.115758</td>\n",
       "      <td>0.955066</td>\n",
       "      <td>0.420882</td>\n",
       "      <td>0.112536</td>\n",
       "      <td>0.956738</td>\n",
       "      <td>0.422481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.111685</td>\n",
       "      <td>0.956814</td>\n",
       "      <td>0.420946</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.957950</td>\n",
       "      <td>0.422508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.111595</td>\n",
       "      <td>0.956826</td>\n",
       "      <td>0.421032</td>\n",
       "      <td>0.134791</td>\n",
       "      <td>0.948509</td>\n",
       "      <td>0.421399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.104994</td>\n",
       "      <td>0.959477</td>\n",
       "      <td>0.421092</td>\n",
       "      <td>0.099097</td>\n",
       "      <td>0.961955</td>\n",
       "      <td>0.422736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.102216</td>\n",
       "      <td>0.960747</td>\n",
       "      <td>0.421154</td>\n",
       "      <td>0.093313</td>\n",
       "      <td>0.964046</td>\n",
       "      <td>0.422097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.091744</td>\n",
       "      <td>0.964775</td>\n",
       "      <td>0.421509</td>\n",
       "      <td>0.089859</td>\n",
       "      <td>0.965386</td>\n",
       "      <td>0.421135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.086134</td>\n",
       "      <td>0.967041</td>\n",
       "      <td>0.421712</td>\n",
       "      <td>0.086178</td>\n",
       "      <td>0.966910</td>\n",
       "      <td>0.423774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.083495</td>\n",
       "      <td>0.968154</td>\n",
       "      <td>0.421970</td>\n",
       "      <td>0.085070</td>\n",
       "      <td>0.967626</td>\n",
       "      <td>0.424144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.082588</td>\n",
       "      <td>0.968431</td>\n",
       "      <td>0.422049</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.967951</td>\n",
       "      <td>0.423050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>0.970717</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.078781</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>0.424286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.079421</td>\n",
       "      <td>0.969591</td>\n",
       "      <td>0.422603</td>\n",
       "      <td>0.073927</td>\n",
       "      <td>0.971764</td>\n",
       "      <td>0.424761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.072772</td>\n",
       "      <td>0.972111</td>\n",
       "      <td>0.423488</td>\n",
       "      <td>0.072942</td>\n",
       "      <td>0.971904</td>\n",
       "      <td>0.423220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.076346</td>\n",
       "      <td>0.970720</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.077586</td>\n",
       "      <td>0.970389</td>\n",
       "      <td>0.424189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.065908</td>\n",
       "      <td>0.974816</td>\n",
       "      <td>0.423997</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.976162</td>\n",
       "      <td>0.423918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.061141</td>\n",
       "      <td>0.976613</td>\n",
       "      <td>0.425052</td>\n",
       "      <td>0.060076</td>\n",
       "      <td>0.976873</td>\n",
       "      <td>0.427904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.059014</td>\n",
       "      <td>0.977405</td>\n",
       "      <td>0.425547</td>\n",
       "      <td>0.068242</td>\n",
       "      <td>0.973557</td>\n",
       "      <td>0.423198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.056601</td>\n",
       "      <td>0.978234</td>\n",
       "      <td>0.426385</td>\n",
       "      <td>0.056121</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>0.429150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.056939</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.426969</td>\n",
       "      <td>0.057704</td>\n",
       "      <td>0.977583</td>\n",
       "      <td>0.425416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.979771</td>\n",
       "      <td>0.428108</td>\n",
       "      <td>0.054154</td>\n",
       "      <td>0.978824</td>\n",
       "      <td>0.429119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>0.977785</td>\n",
       "      <td>0.427158</td>\n",
       "      <td>0.060152</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.426015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.055924</td>\n",
       "      <td>0.978435</td>\n",
       "      <td>0.428063</td>\n",
       "      <td>0.096994</td>\n",
       "      <td>0.962555</td>\n",
       "      <td>0.422515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0      loss  accuracy  mean_io_u_1  val_loss  val_accuracy  \\\n",
       "0            0  0.393351  0.849080     0.420775  0.305916      0.860398   \n",
       "1            1  0.298917  0.872365     0.420775  0.293903      0.874807   \n",
       "2            2  0.274030  0.886187     0.420775  0.254023      0.896829   \n",
       "3            3  0.259818  0.893706     0.420775  0.278415      0.883573   \n",
       "4            4  0.251109  0.898171     0.420775  0.250251      0.895070   \n",
       "5            5  0.240798  0.903103     0.420775  0.235326      0.905231   \n",
       "6            6  0.233087  0.906246     0.420775  0.231087      0.908614   \n",
       "7            7  0.217962  0.912880     0.420775  0.211268      0.915719   \n",
       "8            8  0.213215  0.915348     0.420775  0.208404      0.916180   \n",
       "9            9  0.209126  0.916607     0.420775  0.215834      0.915227   \n",
       "10          10  0.203553  0.918740     0.420775  0.194443      0.922298   \n",
       "11          11  0.199113  0.921280     0.420775  0.188993      0.926336   \n",
       "12          12  0.190084  0.924501     0.420775  0.185193      0.926637   \n",
       "13          13  0.185114  0.926576     0.420775  0.196635      0.921784   \n",
       "14          14  0.187143  0.925883     0.420775  0.176811      0.930917   \n",
       "15          15  0.177641  0.930088     0.420775  0.182740      0.928102   \n",
       "16          16  0.173813  0.931489     0.420775  0.172365      0.932836   \n",
       "17          17  0.171297  0.932424     0.420775  0.170731      0.933085   \n",
       "18          18  0.170946  0.932588     0.420775  0.173109      0.931515   \n",
       "19          19  0.165310  0.935062     0.420775  0.155370      0.938788   \n",
       "20          20  0.158853  0.937438     0.420775  0.155012      0.939444   \n",
       "21          21  0.153971  0.939650     0.420775  0.153901      0.939911   \n",
       "22          22  0.149715  0.941329     0.420775  0.158899      0.938331   \n",
       "23          23  0.146492  0.942929     0.420775  0.138055      0.946200   \n",
       "24          24  0.144577  0.943357     0.420776  0.137464      0.946435   \n",
       "25          25  0.141154  0.944884     0.420793  0.134644      0.947564   \n",
       "26          26  0.132149  0.948744     0.420782  0.128493      0.950665   \n",
       "27          27  0.131533  0.948760     0.420822  0.125392      0.951217   \n",
       "28          28  0.128393  0.950083     0.420850  0.135331      0.947284   \n",
       "29          29  0.121053  0.953270     0.420900  0.114130      0.955946   \n",
       "30          30  0.115758  0.955066     0.420882  0.112536      0.956738   \n",
       "31          31  0.111685  0.956814     0.420946  0.108434      0.957950   \n",
       "32          32  0.111595  0.956826     0.421032  0.134791      0.948509   \n",
       "33          33  0.104994  0.959477     0.421092  0.099097      0.961955   \n",
       "34          34  0.102216  0.960747     0.421154  0.093313      0.964046   \n",
       "35          35  0.091744  0.964775     0.421509  0.089859      0.965386   \n",
       "36          36  0.086134  0.967041     0.421712  0.086178      0.966910   \n",
       "37          37  0.083495  0.968154     0.421970  0.085070      0.967626   \n",
       "38          38  0.082588  0.968431     0.422049  0.083032      0.967951   \n",
       "39          39  0.076582  0.970717     0.422600  0.078781      0.969763   \n",
       "40          40  0.079421  0.969591     0.422603  0.073927      0.971764   \n",
       "41          41  0.072772  0.972111     0.423488  0.072942      0.971904   \n",
       "42          42  0.076346  0.970720     0.423117  0.077586      0.970389   \n",
       "43          43  0.065908  0.974816     0.423997  0.062493      0.976162   \n",
       "44          44  0.061141  0.976613     0.425052  0.060076      0.976873   \n",
       "45          45  0.059014  0.977405     0.425547  0.068242      0.973557   \n",
       "46          46  0.056601  0.978234     0.426385  0.056121      0.978254   \n",
       "47          47  0.056939  0.978052     0.426969  0.057704      0.977583   \n",
       "48          48  0.052590  0.979771     0.428108  0.054154      0.978824   \n",
       "49          49  0.057787  0.977785     0.427158  0.060152      0.976904   \n",
       "50          50  0.055924  0.978435     0.428063  0.096994      0.962555   \n",
       "\n",
       "    val_mean_io_u_1  \n",
       "0          0.422161  \n",
       "1          0.422168  \n",
       "2          0.423341  \n",
       "3          0.422176  \n",
       "4          0.422361  \n",
       "5          0.421864  \n",
       "6          0.421371  \n",
       "7          0.422976  \n",
       "8          0.421225  \n",
       "9          0.422285  \n",
       "10         0.421762  \n",
       "11         0.421555  \n",
       "12         0.422870  \n",
       "13         0.423311  \n",
       "14         0.422199  \n",
       "15         0.422257  \n",
       "16         0.420742  \n",
       "17         0.422574  \n",
       "18         0.421975  \n",
       "19         0.421878  \n",
       "20         0.422438  \n",
       "21         0.422108  \n",
       "22         0.421524  \n",
       "23         0.422158  \n",
       "24         0.422440  \n",
       "25         0.421979  \n",
       "26         0.422195  \n",
       "27         0.421244  \n",
       "28         0.422809  \n",
       "29         0.422581  \n",
       "30         0.422481  \n",
       "31         0.422508  \n",
       "32         0.421399  \n",
       "33         0.422736  \n",
       "34         0.422097  \n",
       "35         0.421135  \n",
       "36         0.423774  \n",
       "37         0.424144  \n",
       "38         0.423050  \n",
       "39         0.424286  \n",
       "40         0.424761  \n",
       "41         0.423220  \n",
       "42         0.424189  \n",
       "43         0.423918  \n",
       "44         0.427904  \n",
       "45         0.423198  \n",
       "46         0.429150  \n",
       "47         0.425416  \n",
       "48         0.429119  \n",
       "49         0.426015  \n",
       "50         0.422515  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./logs/histories/unet_point1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5319b72-9e0f-4eb1-911b-cddefc4fe954",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-194564ddf777b3c0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-194564ddf777b3c0\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
